{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-Cnf45GEK0Ow"
   },
   "outputs": [],
   "source": [
    "# 三元组读取和持久化.rdflib的一项主要的功能就是将一种基于语法（如xml,n3,ntriples,trix,JSON）的文件变换成一个RDF格式的知识\n",
    "# 支持解析和序列化的文件格式：RDF/XML,N3,NTriples,N-Quads,Turtle,TriX,RDFa,Microdata,JSON-LD。\n",
    "from rdflib import Graph \n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import datetime as dt\n",
    "# cPickle可以对任意一种类型的python对象进行序列化操作\n",
    "import cPickle\n",
    "# kitchen includes functions to make gettext easier to use, handling unicode text easier \n",
    "# (conversion with bytes, outputting xml, and calculating how many columns a string takes)\n",
    "from kitchen.text.converters import getwriter, to_bytes, to_unicode\n",
    "# i18n => 国际化：https://pythonhosted.org/kitchen/api-i18n.html\n",
    "from kitchen.i18n import get_translation_object\n",
    "translations = get_translation_object('example')\n",
    "_ = translations.ugettext\n",
    "b_ = translations.lgettext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 2.7.16\n",
      "Tensorflow version: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print('Python version: %s' % platform.python_version())\n",
    "print('Tensorflow version: %s' % tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "05aOlHhgK0O3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N5abe871c1af34c318384399cf826d73c (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Combine two KG \n",
    "## 加载数据集\n",
    "# DWY-NB consists of two datasets DY-NB and DW-NB; each dataset consists of a pair of KGs that can be used for the evaluation of EAtechniques. \n",
    "# The two KGs of DY-NB are subsets of DBpedia [Auer et al., 2007] and Yago [Hoffart et al., 2013], respectively. \n",
    "# The two KGs of DW-NB are subsets of DBpedia and Wikidata [Vrandecic and Krotzsch, 2014].\n",
    "\n",
    "# ttl是Turtle 格式的简称, 是RDF数据的表达格式之一。RDF的最初的的格式是xml/rdf格式，但是过于繁琐，turtle则直观简单很多\n",
    "# Turtle数据描述：\n",
    "# @prefix entity: <http://www.wikidata.org/entity#>\n",
    "# @prefix rdf-schema: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "# @prefix XMLSchema: <http://www.w3.org/2001/XMLSchema#>\n",
    "# entity:Q1376298 rdf-schema:label \"Europe\"\n",
    "# entity:Q5312467 entity:P569 \"1821-09-26\" ^^XMLSchema:date\n",
    "# 其实wd.ttl使用的是ntriples存储\n",
    "lgd_filename = 'DWY-NB/DW-NB/wd.ttl'  # (The subset of Wikidata KG)\n",
    "dbp_filename = 'DWY-NB/DW-NB/dbp_wd.ttl' # (The subset of DBpedia KG)\n",
    "map_file = 'DWY-NB/DW-NB/mapping_wd.ttl' # (The known entity alignment as testing data)\n",
    "\n",
    "# 创建一个图谱\n",
    "graph = Graph()\n",
    "# 解析wd.ttl, dbp_wd.ttl, 文件格式为ntriples\n",
    "graph.parse(location=lgd_filename, format='nt')\n",
    "graph.parse(location=dbp_filename, format='nt')\n",
    "\n",
    "# 解析mapping_wd.ttl\n",
    "map_graph = Graph()\n",
    "map_graph.parse(location=map_file, format='nt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SWJb3cg5K0O-"
   },
   "outputs": [],
   "source": [
    "# 实体标签label\n",
    "entity_label_dict = dict()\n",
    "\n",
    "# 遍历所解析的图谱, 并将头尾实体存储在dict中, 以头为key, 尾为value\n",
    "for s,p,o in graph:\n",
    "    if (unicode)(p) == u'http://www.w3.org/2000/01/rdf-schema#label':\n",
    "        entity_label_dict[s] = (unicode)(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7TufL-_7K0PB"
   },
   "outputs": [],
   "source": [
    "# 统计三元组的头实体个数\n",
    "num_subj_triple = dict()\n",
    "for s,p,o in graph:\n",
    "    if num_subj_triple.get(s) == None:\n",
    "        num_subj_triple[s] = 1\n",
    "    else:\n",
    "        num_subj_triple[s] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Automatically extracted intersection predicates ###\n",
    "# 自动提取相交谓词\n",
    "intersection_predicates = ['http://www.wikidata.org/entity/P36',\\\n",
    "'http://www.wikidata.org/entity/P185',\\\n",
    "'http://www.wikidata.org/entity/P345',\\\n",
    "'http://www.wikidata.org/entity/P214',\\\n",
    "'http://www.wikidata.org/entity/P40',\\\n",
    "'http://www.wikidata.org/entity/P569',\\\n",
    "'http://www.wikidata.org/entity/P102',\\\n",
    "'http://www.wikidata.org/entity/P175',\\\n",
    "'http://www.wikidata.org/entity/P131',\\\n",
    "'http://www.wikidata.org/entity/P577',\\\n",
    "'http://www.wikidata.org/entity/P140',\\\n",
    "'http://www.wikidata.org/entity/P400',\\\n",
    "'http://www.wikidata.org/entity/P736',\\\n",
    "'http://www.wikidata.org/entity/P1432',\\\n",
    "'http://www.wikidata.org/entity/P159',\\\n",
    "'http://www.wikidata.org/entity/P136',\\\n",
    "'http://www.wikidata.org/entity/P1477',\\\n",
    "'http://www.wikidata.org/entity/P227',\\\n",
    "'http://www.wikidata.org/entity/P6',\\\n",
    "'http://www.wikidata.org/entity/P108',\\\n",
    "'http://www.wikidata.org/entity/P585',\\\n",
    "'http://www.wikidata.org/entity/P239',\\\n",
    "'http://www.wikidata.org/entity/P98',\\\n",
    "'http://www.wikidata.org/entity/P54',\\\n",
    "'http://www.wikidata.org/entity/P17',\\\n",
    "'http://www.wikidata.org/entity/P244',\\\n",
    "'http://www.wikidata.org/entity/P238',\\\n",
    "'http://www.wikidata.org/entity/P287',\\\n",
    "'http://www.wikidata.org/entity/P570',\\\n",
    "'http://www.wikidata.org/entity/P176',\\\n",
    "'http://www.wikidata.org/entity/P119',\\\n",
    "'http://www.wikidata.org/entity/P230',\\\n",
    "'http://www.wikidata.org/entity/P50',\\\n",
    "'http://www.wikidata.org/entity/P57',\\\n",
    "'http://www.wikidata.org/entity/P969',\\\n",
    "'http://www.wikidata.org/entity/P20',\\\n",
    "'http://www.wikidata.org/entity/P374',\\\n",
    "'http://www.wikidata.org/entity/P19',\\\n",
    "'http://www.wikidata.org/entity/P84',\\\n",
    "'http://www.wikidata.org/entity/P166',\\\n",
    "'http://www.wikidata.org/entity/P571',\\\n",
    "'http://www.wikidata.org/entity/P184',\\\n",
    "'http://www.wikidata.org/entity/P473',\\\n",
    "'http://www.wikidata.org/entity/P219',\\\n",
    "'http://www.wikidata.org/entity/P170',\\\n",
    "'http://www.wikidata.org/entity/P26',\\\n",
    "'http://www.wikidata.org/entity/P580',\\\n",
    "'http://www.wikidata.org/entity/P1015',\\\n",
    "'http://www.wikidata.org/entity/P408',\\\n",
    "'http://www.wikidata.org/entity/P172',\\\n",
    "'http://www.wikidata.org/entity/P220',\\\n",
    "'http://www.wikidata.org/entity/P177',\\\n",
    "'http://www.wikidata.org/entity/P178',\\\n",
    "'http://www.wikidata.org/entity/P161',\\\n",
    "'http://www.wikidata.org/entity/P27',\\\n",
    "'http://www.wikidata.org/entity/P742',\\\n",
    "'http://www.wikidata.org/entity/P607',\\\n",
    "'http://www.wikidata.org/entity/P286',\\\n",
    "'http://www.wikidata.org/entity/P361',\\\n",
    "'http://www.wikidata.org/entity/P1082',\\\n",
    "'http://www.wikidata.org/entity/P344',\\\n",
    "'http://www.wikidata.org/entity/P106',\\\n",
    "'http://www.wikidata.org/entity/P112',\\\n",
    "'http://www.wikidata.org/entity/P1036',\\\n",
    "'http://www.wikidata.org/entity/P229',\\\n",
    "'http://www.w3.org/2000/01/rdf-schema#label',\\\n",
    "'http://www.wikidata.org/entity/P126',\\\n",
    "'http://www.wikidata.org/entity/P750',\\\n",
    "'http://www.wikidata.org/entity/P144',\\\n",
    "'http://www.wikidata.org/entity/P69',\\\n",
    "'http://www.wikidata.org/entity/P264',\\\n",
    "'http://www.wikidata.org/entity/P218',\\\n",
    "'http://www.wikidata.org/entity/P110',\\\n",
    "'http://www.wikidata.org/entity/P86',\\\n",
    "'http://www.wikidata.org/entity/P957',\\\n",
    "'http://www.wikidata.org/entity/P1040',\\\n",
    "'http://www.wikidata.org/entity/P200',\\\n",
    "'http://www.wikidata.org/entity/P605',\\\n",
    "'http://www.wikidata.org/entity/P118',\\\n",
    "'http://www.wikidata.org/entity/P127']\n",
    "\n",
    "# 相交谓词赋值\n",
    "intersection_predicates_uri = intersection_predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bWXb0oFFK0PE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273161, 1172, 786, 61502)\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "import re # 正则表达式\n",
    "import collections\n",
    "\n",
    "# 字符量长度\n",
    "literal_len = 10\n",
    "\n",
    "# 返回字符类型\n",
    "def dataType(string):\n",
    "    odp='string'\n",
    "    patternBIT=re.compile('[01]') # bit\n",
    "    patternINT=re.compile('[0-9]+') # int\n",
    "    patternFLOAT=re.compile('[0-9]+\\.[0-9]+') # float\n",
    "    patternTEXT=re.compile('[a-zA-Z0-9]+') # text\n",
    "    if patternTEXT.match(string):\n",
    "        odp= \"string\"\n",
    "    if patternINT.match(string):\n",
    "        odp= \"integer\"\n",
    "    if patternFLOAT.match(string):\n",
    "        odp= \"float\"\n",
    "    return odp\n",
    "\n",
    "### Return: data, data_type\n",
    "# 返回data,以及dataType\n",
    "def getRDFData(o):\n",
    "    # 判断是否是URIRef类型\n",
    "    if isinstance(o, rdflib.term.URIRef):\n",
    "        data_type = \"uri\"\n",
    "    else:\n",
    "        data_type = o.datatype\n",
    "        if data_type == None:\n",
    "            data_type = dataType(o) # 得到具体数据类型\n",
    "        else:\n",
    "            if \"#\" in o.datatype:\n",
    "                data_type = o.datatype.split('#')[1].lower() # 以#进行分割, 得到第二部分\n",
    "            else:\n",
    "                data_type = dataType(o)\n",
    "        ## 对时间特殊处理\n",
    "        if data_type == 'gmonthday' or data_type=='gyear':\n",
    "            data_type = 'date'\n",
    "        # 对positiveinteger, nonnegativeinteger特殊处理\n",
    "        if data_type == 'positiveinteger' or data_type == 'int' or data_type == 'nonnegativeinteger':\n",
    "            data_type = 'integer'\n",
    "    return o, data_type\n",
    "\n",
    "# 反转dict：并交换key,value值\n",
    "# iteritems返回迭代器\n",
    "def invert_dict(d):\n",
    "    return dict([(v, k) for k, v in d.iteritems()])\n",
    "\n",
    "# 得到字面量数组：literal_len = 10\n",
    "# o = getRDFData(o), 返回[data, dataType].\n",
    "# char_vocab: 字符向量\n",
    "def getLiteralArray(o, literal_len, char_vocab):\n",
    "    literal_object = list()\n",
    "    # literal_object初始化为0\n",
    "    for i in range(literal_len):\n",
    "        literal_object.append(0)\n",
    "    # 判断数据类型是否是'uri', \n",
    "    # 是字面量, 则对字面量进行处理\n",
    "    if o[1] != 'uri':\n",
    "        max_len = min(literal_len, len(o[0]))\n",
    "        for i in range(max_len):\n",
    "            # char_vocab没有字符o[0][i]对应的字符向量\n",
    "            if char_vocab.get(o[0][i]) == None:\n",
    "                char_vocab[o[0][i]] = len(char_vocab) # 字符向量为len(char_vocab)\n",
    "            literal_object[i] = char_vocab[o[0][i]] \n",
    "    # 是'uri', 并且o是头实体,通过entity_label_dict得到尾实体\n",
    "    elif entity_label_dict.get(o[0]) != None:\n",
    "        label = entity_label_dict.get(o[0])\n",
    "        max_len = min(literal_len, len(label))\n",
    "        for i in range(max_len):\n",
    "            # char_vocab没有字符label[i]对应的字符向量\n",
    "            if char_vocab.get(label[i]) == None:\n",
    "                char_vocab[label[i]] = len(char_vocab)\n",
    "            literal_object[i] = char_vocab[label[i]]\n",
    "        \n",
    "    return literal_object\n",
    "\n",
    "# entity 词向量\n",
    "entity_vocab = dict()\n",
    "# dbp entity词向量\n",
    "entity_dbp_vocab = list()\n",
    "\n",
    "# 实体词向量的负样本\n",
    "entity_dbp_vocab_neg = list()\n",
    "entity_lgd_vocab_neg = list()\n",
    "\n",
    "# 谓词向量\n",
    "predicate_vocab = dict()\n",
    "predicate_vocab['<NONE>'] = 0\n",
    "\n",
    "# 实体字面量词向量\n",
    "entity_literal_vocab = dict()\n",
    "# 实体字面量的负样本\n",
    "entity_literal_dbp_vocab_neg = list()\n",
    "entity_literal_lgd_vocab_neg = list()\n",
    "\n",
    "# 存储uri类型的三元组数据\n",
    "data_uri = [] ###[ [[s,p,o,p_trans],[chars],predicate_weight], ... ]\n",
    "data_uri_0 = []\n",
    "\n",
    "# 存储字面量类型的三元组数据\n",
    "data_literal_0 = []\n",
    "data_literal = []\n",
    "\n",
    "# 存储进行了谓词转换后的数据\n",
    "data_uri_trans = []\n",
    "data_literal_trans = []\n",
    "\n",
    "# 字符向量\n",
    "char_vocab = dict()\n",
    "char_vocab['<pad>'] = 0\n",
    "#tmp_data = []\n",
    "\n",
    "# 实体权重\n",
    "pred_weight = dict()\n",
    "\n",
    "# 三元组个数\n",
    "num_triples = 0\n",
    "\n",
    "for s, p, o in graph:\n",
    "    \n",
    "    num_triples += 1\n",
    "    \n",
    "    # [data, data_type]\n",
    "    s = getRDFData(s)\n",
    "    p = getRDFData(p)\n",
    "    o = getRDFData(o)\n",
    "    \n",
    "    # 更新谓词权重\n",
    "    if pred_weight.get(p[0]) == None:\n",
    "        pred_weight[p[0]] = 1\n",
    "    else:\n",
    "        pred_weight[p[0]] += 1\n",
    "\n",
    "    ## all vocab for finding neg sample => 寻找负样本的所有词汇\n",
    "    # 设置头实体s\n",
    "    if entity_literal_vocab.get(s[0]) == None:\n",
    "        # 设置实体字面量词向量：词向量为len(entity_literal_vocab)\n",
    "        entity_literal_vocab[s[0]] = len(entity_literal_vocab) \n",
    "        if (unicode)(s[0]).startswith(u'http://dbpedia.org/resource/'):\n",
    "            entity_literal_dbp_vocab_neg.append(s[0]) # dbp_vocab_neg\n",
    "        else:\n",
    "            entity_literal_lgd_vocab_neg.append(s[0]) # lgd_vocab_neg\n",
    "    \n",
    "    # 设置尾实体o\n",
    "    if entity_literal_vocab.get(o[0]) == None:\n",
    "        # 词向量为len(entity_literal_vocab) \n",
    "        entity_literal_vocab[o[0]] = len(entity_literal_vocab)\n",
    "        if (unicode)(s[0]).startswith(u'http://dbpedia.org/resource/'):\n",
    "            entity_literal_dbp_vocab_neg.append(o[0]) # dbp_vocab_neg\n",
    "        else:\n",
    "            entity_literal_lgd_vocab_neg.append(o[0]) # lgd_vocab_neg\n",
    "    \n",
    "    # 设置头实体s词向量\n",
    "    if entity_vocab.get(s[0]) == None:\n",
    "        idx = len(entity_vocab)\n",
    "        entity_vocab[s[0]] = idx # 实体词向量为len(entity_vocab)\n",
    "        if (unicode)(s[0]).startswith(u'http://dbpedia.org/resource/'):\n",
    "            entity_dbp_vocab.append(idx)\n",
    "            entity_dbp_vocab_neg.append(s[0])\n",
    "        else:\n",
    "            entity_lgd_vocab_neg.append(s[0])\n",
    "    \n",
    "    # 设置谓词向量 => len(predicate_vocab)\n",
    "    if predicate_vocab.get(p[0]) == None:\n",
    "        predicate_vocab[p[0]] = len(predicate_vocab)\n",
    "    \n",
    "    # 数据类型为'uri' => 即尾实体还是一个关系, 设置尾实体的词向量\n",
    "    if o[1] == 'uri':\n",
    "        if entity_vocab.get(o[0]) == None:\n",
    "            entity_vocab[o[0]] = len(entity_vocab)\n",
    "            if (unicode)(s[0]).startswith(u'http://dbpedia.org/resource/'):\n",
    "                entity_dbp_vocab_neg.append(o[0]) # dbp_vocab_neg\n",
    "            else:\n",
    "                entity_lgd_vocab_neg.append(o[0]) # lgd_vocab_neg\n",
    "        # 得到字面量对象, 并根据o更新char_vocab, 对尾实体o进行字符级别处理\n",
    "        literal_object = getLiteralArray(o, literal_len, char_vocab)\n",
    "        # 若当前谓词不在相交谓词中, 则将对应的实体向量进行存储data_uri_0\n",
    "        if (unicode)(p[0]) not in intersection_predicates_uri:\n",
    "            data_uri_0.append([[entity_vocab[s[0]], predicate_vocab[p[0]], entity_vocab[o[0]], 0], literal_object])\n",
    "        else:\n",
    "             # 若当前谓词在相交谓词中, 则将对应的实体向量进行存储data_uri\n",
    "            data_uri.append([[entity_vocab[s[0]], predicate_vocab[p[0]], entity_vocab[o[0]], 0], literal_object])\n",
    "            ### DATA TRANS => 数据转换\n",
    "            # 找到重复谓词\n",
    "            # Counter统计字符重复次数\n",
    "            # A generator of predicates with the given subject and object\n",
    "            duplicate_preds = [item for item, count in collections.Counter(graph.predicates(o[0],None)).items() if count > 1]\n",
    "            if True:\n",
    "              # 找到以o[0]开头的三元组\n",
    "                for g1 in graph.triples((o[0],None,None)):\n",
    "                    if len(g1) > 0:\n",
    "                        s1,p1,o1 = g1\n",
    "\n",
    "                        s1 = getRDFData(s1)\n",
    "                        p1 = getRDFData(p1)\n",
    "                        o1 = getRDFData(o1)\n",
    "                        \n",
    "                        # entity 以及谓词词向量处理 \n",
    "                        if entity_vocab.get(o1[0]) == None:\n",
    "                            entity_vocab[o1[0]] = len(entity_vocab)\n",
    "                        if (unicode)(s1[0]).startswith(u'http://dbpedia.org/resource/'):\n",
    "                            entity_dbp_vocab_neg.append(o1[0])\n",
    "                        else:\n",
    "                            entity_lgd_vocab_neg.append(o1[0])\n",
    "                            \n",
    "                        if entity_vocab.get(o1[1]) == None:\n",
    "                            entity_vocab[o1[1]] = len(entity_vocab)\n",
    "                        if predicate_vocab.get(p1[0]) == None:\n",
    "                            predicate_vocab[p1[0]] = len(predicate_vocab)\n",
    "                        \n",
    "                        # 两个谓词不相等, 并且与intersection_predicates_uri存在交集\n",
    "                        if p[0] != p1[0] \\\n",
    "                            and len(set((unicode)(x) for x in (graph.predicates(s[0]))).intersection(set(intersection_predicates_uri))) != 0:\n",
    "                            # 头实体为URIRef 并且谓词存在于intersection_predicates_uri. \n",
    "                            # 存储转换内容到isinstance\n",
    "                            if isinstance(o1[0], rdflib.term.URIRef) and (unicode)(p1[0]) in intersection_predicates_uri:\n",
    "                                data_uri_trans.append([[entity_vocab[s[0]], predicate_vocab[p[0]], entity_vocab[o1[0]], predicate_vocab[p1[0]]], getLiteralArray(o1, literal_len, char_vocab)])\n",
    "                            # 头实体为URIRef 并且谓词存为某个特定值 \n",
    "                            elif isinstance(o1[0], rdflib.term.Literal) and (unicode)(p1[0]) == u'http://www.w3.org/2000/01/rdf-schema#label':\n",
    "                                data_literal_trans.append([[entity_vocab[s[0]], predicate_vocab[p[0]], entity_vocab[o1[1]], predicate_vocab[p1[0]]], getLiteralArray(o1, literal_len, char_vocab)])\n",
    "                              #tmp_data.append((s[0], p[0], o[0], p1[0], o1[0]))\n",
    "              ##############\n",
    "    else:\n",
    "        # 数据类型为字面量, 进行字面量的处理\n",
    "        if entity_vocab.get(o[1]) == None:\n",
    "            entity_vocab[o[1]] = len(entity_vocab)\n",
    "        # 得到字面量对象, 并根据o更新char_vocab, 对字面量进行字符级别处理\n",
    "        literal_object = getLiteralArray(o, literal_len, char_vocab)\n",
    "        # 若当前谓词不在相交谓词中, 则将对应的实体向量进行存储data_literal_0\n",
    "        if (unicode)(p[0]) not in intersection_predicates:\n",
    "            data_literal_0.append([[entity_vocab[s[0]], predicate_vocab[p[0]], entity_vocab[o[1]], 0], literal_object])\n",
    "        else:\n",
    "            # 若当前谓词不在相交谓词中, 则将对应的实体向量进行存储data_literal\n",
    "            data_literal.append([[entity_vocab[s[0]], predicate_vocab[p[0]], entity_vocab[o[1]], 0], literal_object])\n",
    "\n",
    "# 词向量翻转\n",
    "reverse_entity_vocab = invert_dict(entity_vocab)\n",
    "reverse_predicate_vocab = invert_dict(predicate_vocab)\n",
    "reverse_char_vocab = invert_dict(char_vocab)\n",
    "reverse_entity_literal_vocab = invert_dict(entity_literal_vocab)\n",
    "\n",
    "#Add predicate weight => 增加谓词权重\n",
    "\n",
    "for i in range(0, len(data_uri)):\n",
    "    # data_uri = [] ###[ [[s,p,o,p_trans],[chars],predicate_weight], ... ]\n",
    "    s = reverse_entity_vocab.get(data_uri[i][0][0])\n",
    "    p = reverse_predicate_vocab.get(data_uri[i][0][1])\n",
    "    # 谓词权重 = 出现次数 / 三元组总数\n",
    "    data_uri[i].append([(pred_weight.get(p)/float(num_triples))])\n",
    "\n",
    "# 下面的转换过程同上\n",
    "for i in range(0, len(data_uri_0)):\n",
    "    s = reverse_entity_vocab.get(data_uri_0[i][0][0])\n",
    "    p = reverse_predicate_vocab.get(data_uri_0[i][0][1])\n",
    "    data_uri_0[i].append([(pred_weight.get(p)/float(num_triples))])\n",
    "\n",
    "for i in range(0, len(data_uri_trans)):\n",
    "    s = reverse_entity_vocab.get(data_uri_trans[i][0][0])\n",
    "    p = reverse_predicate_vocab.get(data_uri_trans[i][0][1])\n",
    "    data_uri_trans[i].append([(pred_weight.get(p)/float(num_triples))])\n",
    "    \n",
    "for i in range(0, len(data_literal)):\n",
    "    s = reverse_entity_vocab.get(data_literal[i][0][0])\n",
    "    p = reverse_predicate_vocab.get(data_literal[i][0][1])\n",
    "    data_literal[i].append([(pred_weight.get(p)/float(num_triples))])\n",
    "\n",
    "for i in range(0, len(data_literal_0)):\n",
    "    s = reverse_entity_vocab.get(data_literal_0[i][0][0])\n",
    "    p = reverse_predicate_vocab.get(data_literal_0[i][0][1])\n",
    "    data_literal_0[i].append([(pred_weight.get(p)/float(num_triples))])\n",
    "    \n",
    "for i in range(0, len(data_literal_trans)):\n",
    "    s = reverse_entity_vocab.get(data_literal_trans[i][0][0])\n",
    "    p = reverse_predicate_vocab.get(data_literal_trans[i][0][1])\n",
    "    data_literal_trans[i].append([(pred_weight.get(p)/float(num_triples))])\n",
    "    \n",
    "if len(data_uri_trans) < 100:\n",
    "    data_uri_trans = data_uri_trans+data_uri_trans\n",
    "    \n",
    "print (len(entity_vocab), len(predicate_vocab), len(char_vocab), len(entity_dbp_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对上述向量进行持久化操作 - pickle模块实现了基本的数据序列化和反序列化。\n",
    "# 通过pickle模块的序列化操作我们能够将程序中运行的对象信息保存到文件中去，永久存储\n",
    "cPickle.dump(entity_literal_vocab, open(\"data/vocab_all.pickle\", \"wb\")) \n",
    "cPickle.dump(char_vocab, open(\"data/vocab_char.pickle\", \"wb\"))\n",
    "cPickle.dump(entity_vocab, open(\"data/vocab_entity.pickle\", \"wb\")) \n",
    "cPickle.dump(predicate_vocab, open(\"data/vocab_predicate.pickle\", \"wb\")) \n",
    "cPickle.dump(entity_dbp_vocab, open(\"data/vocab_kb1.pickle\", \"wb\")) \n",
    "cPickle.dump(entity_dbp_vocab_neg, open(\"data/vocab_kb1_neg.pickle\", \"wb\")) \n",
    "cPickle.dump(entity_lgd_vocab_neg, open(\"data/vocab_kb2_neg.pickle\", \"wb\")) \n",
    "cPickle.dump(entity_label_dict, open(\"data/entity_label.pickle\", \"wb\")) \n",
    "cPickle.dump(entity_literal_dbp_vocab_neg, open(\"data/vocab_kb1_all_neg.pickle\", \"wb\")) \n",
    "cPickle.dump(entity_literal_lgd_vocab_neg, open(\"data/vocab_kb2_all_neg.pickle\", \"wb\")) \n",
    "cPickle.dump(data_uri, open(\"data/data_uri.pickle\", \"wb\"))\n",
    "cPickle.dump(data_uri_0, open(\"data/data_uri_n.pickle\", \"wb\"))\n",
    "cPickle.dump(data_literal, open(\"data/data_literal.pickle\", \"wb\"))\n",
    "cPickle.dump(data_literal_0, open(\"data/data_literal_n.pickle\", \"wb\"))\n",
    "cPickle.dump(data_uri_trans, open(\"data/data_trans.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "TextKE6(TRANS).ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
